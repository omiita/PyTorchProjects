{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import glob","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '../input/cifar/train/*.png'\ntest_path = '../input/cifar/test/*.png'\n\ntrain_imgs = glob.glob(train_path)\ntest_imgs = glob.glob(test_path)\nlabels = []\nwith open(\"../input/cifar/labels.txt\", 'r') as f:\n    labels = f.readlines()\nlabel_idx_dict = {label.replace('\\n',''):idx for idx, label in enumerate(labels)}\nlabel_idx_dict","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"{'airplane': 0,\n 'automobile': 1,\n 'bird': 2,\n 'cat': 3,\n 'deer': 4,\n 'dog': 5,\n 'frog': 6,\n 'horse': 7,\n 'ship': 8,\n 'truck': 9}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport PIL\nimport time","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_dict = {\n    'train':torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ]),\n    'test':torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n}","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CIFAR10Dataset(torch.utils.data.Dataset):\n    def __init__(self,img_path, label_idx_dict,transform=None):\n        super(CIFAR10Dataset,self).__init__()\n        self.img_path = img_path\n        self.label_idx_dict = label_idx_dict\n        self.transform = transform\n        \n    def __getitem__(self,idx):\n        img_p = self.img_path[idx]\n        img = PIL.Image.open(img_p)\n        label = img_p.split('/')[-1].split('_')[-1].replace('.png','')\n        label_idx = self.label_idx_dict[label]\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label_idx\n        \n    def __len__(self):\n        return len(self.img_path)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = CIFAR10Dataset(train_imgs, label_idx_dict, transform_dict['train'])\ntest_ds = CIFAR10Dataset(test_imgs, label_idx_dict, transform_dict['test'])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1024\nnum_epoch = 200","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = torchvision.models.mobilenet_v2(pretrained=True)\nnet.classifier[1] = torch.nn.Linear(1280,10)\nmodel_name = net.__class__.__name__\nnet","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"MobileNetV2(\n  (features): Sequential(\n    (0): ConvBNReLU(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (9): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (16): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (17): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (18): ConvBNReLU(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=1280, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = torchvision.models.resnet50(pretrained=True)\nnet.fc = torch.nn.Linear(2048, 10)\nnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(),lr=5e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=num_epoch)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ntorch.backends.cudnn.benchmark = True\nnet.to(device)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"MobileNetV2(\n  (features): Sequential(\n    (0): ConvBNReLU(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (9): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (16): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (17): InvertedResidual(\n      (conv): Sequential(\n        (0): ConvBNReLU(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): ConvBNReLU(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (18): ConvBNReLU(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=1280, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\nprint(\"MODEL: {}\".format(model_name))\nfor epoch in range(num_epoch):\n    since = time.time()\n    epoch_loss = 0.\n    epoch_corrects = 0.\n    epoch_imgs = 0.\n    \n    for batch in train_dl:\n        imgs, labels = batch\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        epoch_imgs += len(imgs)\n        \n        optimizer.zero_grad()\n        outputs=net(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item() * imgs.size(0)\n        epoch_corrects += sum(outputs.argmax(1)==labels)\n        \n    l = epoch_loss / epoch_imgs\n    a = epoch_corrects / epoch_imgs\n\n    hist['loss'].append(l)\n    hist['acc'].append(a)\n    \n    print(\"Epoch {}: Loss: {:.4f} Acc: {:.4} Time: {}\".format(epoch+1,l, a, time.time()-since))\n    \n    \n    net.eval()\n    val_loss = 0.\n    val_corrects = 0.\n    val_imgs = 0.\n    \n    for batch in test_dl:\n        imgs, labels = batch\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        val_imgs += len(labels)\n        \n        with torch.no_grad():\n            outputs = net(imgs)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()*imgs.size(0)\n            val_corrects += sum(outputs.argmax(1) == labels)\n            \n    l = val_loss / val_imgs\n    a = val_corrects / val_imgs\n    cur_lr = scheduler.optimizer.param_groups[0]['lr']\n    \n    hist['val_loss'].append(l)\n    hist['val_acc'].append(a)\n    \n    print(\"Val: Loss: {:.4f} Acc: {:.4f} lr: {:.6f}\".format(l, a, cur_lr))\n    \n    scheduler.step()","execution_count":null,"outputs":[{"output_type":"stream","text":"MODEL: MobileNetV2\nEpoch 1: Loss: 1.7748 Acc: 0.3757 Time: 43.0116400718689\nVal: Loss: 1.3341 Acc: 0.5303 lr: 0.000050\nEpoch 2: Loss: 1.1094 Acc: 0.605 Time: 46.88619947433472\nVal: Loss: 1.0313 Acc: 0.6335 lr: 0.000050\nEpoch 3: Loss: 0.9126 Acc: 0.6769 Time: 44.9997763633728\nVal: Loss: 0.9380 Acc: 0.6702 lr: 0.000050\nEpoch 4: Loss: 0.8044 Acc: 0.7168 Time: 47.45250487327576\nVal: Loss: 0.8854 Acc: 0.6856 lr: 0.000050\nEpoch 5: Loss: 0.7205 Acc: 0.7469 Time: 46.69953894615173\nVal: Loss: 0.8602 Acc: 0.6994 lr: 0.000050\nEpoch 6: Loss: 0.6506 Acc: 0.7704 Time: 45.68670964241028\nVal: Loss: 0.8325 Acc: 0.7100 lr: 0.000050\nEpoch 7: Loss: 0.5893 Acc: 0.7935 Time: 44.32051682472229\nVal: Loss: 0.8258 Acc: 0.7182 lr: 0.000050\nEpoch 8: Loss: 0.5282 Acc: 0.8164 Time: 46.67786121368408\nVal: Loss: 0.8261 Acc: 0.7205 lr: 0.000050\nEpoch 9: Loss: 0.4814 Acc: 0.8342 Time: 46.08226156234741\nVal: Loss: 0.8334 Acc: 0.7230 lr: 0.000050\nEpoch 10: Loss: 0.4284 Acc: 0.8532 Time: 46.18557786941528\nVal: Loss: 0.8277 Acc: 0.7312 lr: 0.000050\nEpoch 11: Loss: 0.3796 Acc: 0.872 Time: 47.06821346282959\nVal: Loss: 0.8444 Acc: 0.7338 lr: 0.000050\nEpoch 12: Loss: 0.3302 Acc: 0.8921 Time: 46.407747983932495\nVal: Loss: 0.8720 Acc: 0.7331 lr: 0.000050\nEpoch 13: Loss: 0.2874 Acc: 0.9089 Time: 46.7261745929718\nVal: Loss: 0.9052 Acc: 0.7282 lr: 0.000050\nEpoch 14: Loss: 0.2436 Acc: 0.9266 Time: 47.12221360206604\nVal: Loss: 0.9327 Acc: 0.7281 lr: 0.000049\nEpoch 15: Loss: 0.2003 Acc: 0.9443 Time: 46.63414907455444\nVal: Loss: 0.9690 Acc: 0.7309 lr: 0.000049\nEpoch 16: Loss: 0.1631 Acc: 0.9579 Time: 47.53137826919556\nVal: Loss: 1.0188 Acc: 0.7291 lr: 0.000049\nEpoch 17: Loss: 0.1270 Acc: 0.971 Time: 48.82217311859131\nVal: Loss: 1.0780 Acc: 0.7260 lr: 0.000049\nEpoch 18: Loss: 0.0982 Acc: 0.9806 Time: 48.65397071838379\nVal: Loss: 1.1390 Acc: 0.7271 lr: 0.000049\nEpoch 19: Loss: 0.0714 Acc: 0.9891 Time: 49.034377574920654\nVal: Loss: 1.1902 Acc: 0.7265 lr: 0.000049\nEpoch 20: Loss: 0.0520 Acc: 0.9936 Time: 46.97482991218567\nVal: Loss: 1.2593 Acc: 0.7284 lr: 0.000049\nEpoch 21: Loss: 0.0370 Acc: 0.9965 Time: 47.993046283721924\nVal: Loss: 1.3196 Acc: 0.7260 lr: 0.000049\nEpoch 22: Loss: 0.0275 Acc: 0.998 Time: 46.992793560028076\nVal: Loss: 1.3751 Acc: 0.7244 lr: 0.000049\nEpoch 23: Loss: 0.0199 Acc: 0.9991 Time: 46.95808982849121\nVal: Loss: 1.4279 Acc: 0.7260 lr: 0.000049\nEpoch 24: Loss: 0.0144 Acc: 0.9995 Time: 48.09715795516968\nVal: Loss: 1.4720 Acc: 0.7271 lr: 0.000048\nEpoch 25: Loss: 0.0110 Acc: 0.9998 Time: 48.917442083358765\nVal: Loss: 1.5160 Acc: 0.7252 lr: 0.000048\nEpoch 26: Loss: 0.0088 Acc: 0.9999 Time: 47.91113471984863\nVal: Loss: 1.5542 Acc: 0.7261 lr: 0.000048\nEpoch 27: Loss: 0.0071 Acc: 0.9999 Time: 48.49999761581421\nVal: Loss: 1.5873 Acc: 0.7260 lr: 0.000048\nEpoch 28: Loss: 0.0059 Acc: 1.0 Time: 46.583860874176025\nVal: Loss: 1.6184 Acc: 0.7264 lr: 0.000048\nEpoch 29: Loss: 0.0050 Acc: 1.0 Time: 47.06081676483154\nVal: Loss: 1.6466 Acc: 0.7259 lr: 0.000048\nEpoch 30: Loss: 0.0043 Acc: 1.0 Time: 49.43876075744629\nVal: Loss: 1.6753 Acc: 0.7254 lr: 0.000047\nEpoch 31: Loss: 0.0038 Acc: 1.0 Time: 47.921062707901\nVal: Loss: 1.6987 Acc: 0.7255 lr: 0.000047\nEpoch 32: Loss: 0.0033 Acc: 1.0 Time: 48.36612248420715\nVal: Loss: 1.7214 Acc: 0.7261 lr: 0.000047\nEpoch 33: Loss: 0.0030 Acc: 1.0 Time: 48.579981088638306\nVal: Loss: 1.7426 Acc: 0.7265 lr: 0.000047\nEpoch 34: Loss: 0.0027 Acc: 1.0 Time: 48.17391109466553\nVal: Loss: 1.7633 Acc: 0.7254 lr: 0.000047\nEpoch 35: Loss: 0.0024 Acc: 1.0 Time: 48.640082120895386\nVal: Loss: 1.7833 Acc: 0.7255 lr: 0.000047\nEpoch 36: Loss: 0.0022 Acc: 1.0 Time: 48.01134538650513\nVal: Loss: 1.8021 Acc: 0.7266 lr: 0.000046\nEpoch 37: Loss: 0.0020 Acc: 1.0 Time: 47.569133281707764\nVal: Loss: 1.8206 Acc: 0.7268 lr: 0.000046\nEpoch 38: Loss: 0.0018 Acc: 1.0 Time: 47.7933874130249\nVal: Loss: 1.8356 Acc: 0.7252 lr: 0.000046\nEpoch 39: Loss: 0.0017 Acc: 1.0 Time: 47.28131341934204\nVal: Loss: 1.8516 Acc: 0.7268 lr: 0.000046\nEpoch 40: Loss: 0.0016 Acc: 1.0 Time: 47.33902883529663\nVal: Loss: 1.8663 Acc: 0.7266 lr: 0.000045\nEpoch 41: Loss: 0.0014 Acc: 1.0 Time: 49.18906044960022\nVal: Loss: 1.8810 Acc: 0.7260 lr: 0.000045\nEpoch 42: Loss: 0.0013 Acc: 1.0 Time: 48.72313117980957\nVal: Loss: 1.8960 Acc: 0.7254 lr: 0.000045\nEpoch 43: Loss: 0.0012 Acc: 1.0 Time: 49.269601821899414\nVal: Loss: 1.9092 Acc: 0.7256 lr: 0.000045\nEpoch 44: Loss: 0.0012 Acc: 1.0 Time: 50.34379482269287\nVal: Loss: 1.9237 Acc: 0.7254 lr: 0.000045\nEpoch 45: Loss: 0.0011 Acc: 1.0 Time: 50.19814491271973\nVal: Loss: 1.9360 Acc: 0.7259 lr: 0.000044\nEpoch 46: Loss: 0.0010 Acc: 1.0 Time: 49.12048840522766\nVal: Loss: 1.9483 Acc: 0.7262 lr: 0.000044\nEpoch 47: Loss: 0.0010 Acc: 1.0 Time: 48.02015018463135\nVal: Loss: 1.9601 Acc: 0.7264 lr: 0.000044\nEpoch 48: Loss: 0.0009 Acc: 1.0 Time: 48.08695697784424\nVal: Loss: 1.9731 Acc: 0.7268 lr: 0.000043\nEpoch 49: Loss: 0.0008 Acc: 1.0 Time: 47.39236664772034\nVal: Loss: 1.9841 Acc: 0.7265 lr: 0.000043\nEpoch 50: Loss: 0.0008 Acc: 1.0 Time: 48.075610637664795\nVal: Loss: 1.9963 Acc: 0.7273 lr: 0.000043\nEpoch 51: Loss: 0.0008 Acc: 1.0 Time: 49.616217374801636\nVal: Loss: 2.0072 Acc: 0.7269 lr: 0.000043\nEpoch 52: Loss: 0.0007 Acc: 1.0 Time: 50.17433762550354\nVal: Loss: 2.0177 Acc: 0.7261 lr: 0.000042\nEpoch 53: Loss: 0.0007 Acc: 1.0 Time: 48.619845151901245\nVal: Loss: 2.0275 Acc: 0.7262 lr: 0.000042\nEpoch 54: Loss: 0.0006 Acc: 1.0 Time: 48.78583312034607\nVal: Loss: 2.0374 Acc: 0.7260 lr: 0.000042\nEpoch 55: Loss: 0.0006 Acc: 1.0 Time: 48.77349233627319\nVal: Loss: 2.0483 Acc: 0.7266 lr: 0.000042\nEpoch 56: Loss: 0.0006 Acc: 1.0 Time: 48.584606409072876\nVal: Loss: 2.0578 Acc: 0.7270 lr: 0.000041\nEpoch 57: Loss: 0.0006 Acc: 1.0 Time: 50.75489282608032\nVal: Loss: 2.0671 Acc: 0.7269 lr: 0.000041\nEpoch 58: Loss: 0.0005 Acc: 1.0 Time: 49.96318054199219\nVal: Loss: 2.0768 Acc: 0.7268 lr: 0.000041\nEpoch 59: Loss: 0.0005 Acc: 1.0 Time: 49.772393465042114\nVal: Loss: 2.0856 Acc: 0.7263 lr: 0.000040\nEpoch 60: Loss: 0.0005 Acc: 1.0 Time: 49.870095014572144\nVal: Loss: 2.0944 Acc: 0.7268 lr: 0.000040\nEpoch 61: Loss: 0.0005 Acc: 1.0 Time: 50.22479033470154\nVal: Loss: 2.1032 Acc: 0.7268 lr: 0.000040\nEpoch 62: Loss: 0.0004 Acc: 1.0 Time: 48.837568283081055\nVal: Loss: 2.1124 Acc: 0.7268 lr: 0.000039\nEpoch 63: Loss: 0.0004 Acc: 1.0 Time: 50.056910276412964\nVal: Loss: 2.1205 Acc: 0.7266 lr: 0.000039\nEpoch 64: Loss: 0.0004 Acc: 1.0 Time: 49.53166723251343\nVal: Loss: 2.1285 Acc: 0.7271 lr: 0.000039\nEpoch 65: Loss: 0.0004 Acc: 1.0 Time: 48.27307677268982\nVal: Loss: 2.1362 Acc: 0.7272 lr: 0.000038\nEpoch 66: Loss: 0.0004 Acc: 1.0 Time: 48.81311106681824\nVal: Loss: 2.1443 Acc: 0.7267 lr: 0.000038\nEpoch 67: Loss: 0.0004 Acc: 1.0 Time: 49.623257875442505\nVal: Loss: 2.1524 Acc: 0.7270 lr: 0.000038\nEpoch 68: Loss: 0.0003 Acc: 1.0 Time: 49.95686078071594\nVal: Loss: 2.1605 Acc: 0.7272 lr: 0.000037\nEpoch 69: Loss: 0.0003 Acc: 1.0 Time: 48.38135576248169\nVal: Loss: 2.1674 Acc: 0.7264 lr: 0.000037\nEpoch 70: Loss: 0.0003 Acc: 1.0 Time: 48.59548592567444\nVal: Loss: 2.1753 Acc: 0.7266 lr: 0.000037\nEpoch 71: Loss: 0.0003 Acc: 1.0 Time: 48.71262168884277\nVal: Loss: 2.1826 Acc: 0.7265 lr: 0.000036\nEpoch 72: Loss: 0.0003 Acc: 1.0 Time: 49.02990245819092\nVal: Loss: 2.1900 Acc: 0.7267 lr: 0.000036\nEpoch 73: Loss: 0.0003 Acc: 1.0 Time: 50.50146794319153\nVal: Loss: 2.1971 Acc: 0.7269 lr: 0.000036\nEpoch 74: Loss: 0.0003 Acc: 1.0 Time: 49.23540949821472\nVal: Loss: 2.2036 Acc: 0.7267 lr: 0.000035\nEpoch 75: Loss: 0.0003 Acc: 1.0 Time: 49.60548114776611\nVal: Loss: 2.2102 Acc: 0.7272 lr: 0.000035\nEpoch 76: Loss: 0.0003 Acc: 1.0 Time: 47.31514501571655\nVal: Loss: 2.2169 Acc: 0.7268 lr: 0.000035\nEpoch 77: Loss: 0.0002 Acc: 1.0 Time: 47.93737983703613\nVal: Loss: 2.2241 Acc: 0.7270 lr: 0.000034\nEpoch 78: Loss: 0.0002 Acc: 1.0 Time: 47.87635111808777\nVal: Loss: 2.2301 Acc: 0.7263 lr: 0.000034\nEpoch 79: Loss: 0.0002 Acc: 1.0 Time: 48.972455739974976\nVal: Loss: 2.2372 Acc: 0.7269 lr: 0.000033\nEpoch 80: Loss: 0.0002 Acc: 1.0 Time: 50.11423182487488\nVal: Loss: 2.2442 Acc: 0.7273 lr: 0.000033\nEpoch 81: Loss: 0.0002 Acc: 1.0 Time: 48.41199588775635\nVal: Loss: 2.2499 Acc: 0.7264 lr: 0.000033\nEpoch 82: Loss: 0.0002 Acc: 1.0 Time: 49.01201295852661\nVal: Loss: 2.2560 Acc: 0.7275 lr: 0.000032\n","name":"stdout"},{"output_type":"stream","text":"Epoch 83: Loss: 0.0002 Acc: 1.0 Time: 49.17996573448181\nVal: Loss: 2.2623 Acc: 0.7268 lr: 0.000032\nEpoch 84: Loss: 0.0002 Acc: 1.0 Time: 50.6673309803009\nVal: Loss: 2.2683 Acc: 0.7265 lr: 0.000032\nEpoch 85: Loss: 0.0002 Acc: 1.0 Time: 49.31101655960083\nVal: Loss: 2.2747 Acc: 0.7265 lr: 0.000031\nEpoch 86: Loss: 0.0002 Acc: 1.0 Time: 48.22125864028931\nVal: Loss: 2.2795 Acc: 0.7274 lr: 0.000031\nEpoch 87: Loss: 0.0002 Acc: 1.0 Time: 48.437172651290894\nVal: Loss: 2.2860 Acc: 0.7269 lr: 0.000030\nEpoch 88: Loss: 0.0002 Acc: 1.0 Time: 49.426344871520996\nVal: Loss: 2.2913 Acc: 0.7268 lr: 0.000030\nEpoch 89: Loss: 0.0002 Acc: 1.0 Time: 49.47222423553467\nVal: Loss: 2.2971 Acc: 0.7269 lr: 0.000030\nEpoch 90: Loss: 0.0002 Acc: 1.0 Time: 48.13154053688049\nVal: Loss: 2.3030 Acc: 0.7261 lr: 0.000029\nEpoch 91: Loss: 0.0002 Acc: 1.0 Time: 48.14185643196106\nVal: Loss: 2.3083 Acc: 0.7264 lr: 0.000029\nEpoch 92: Loss: 0.0002 Acc: 1.0 Time: 47.74266791343689\nVal: Loss: 2.3137 Acc: 0.7273 lr: 0.000029\nEpoch 93: Loss: 0.0002 Acc: 1.0 Time: 48.925941944122314\nVal: Loss: 2.3194 Acc: 0.7276 lr: 0.000028\nEpoch 94: Loss: 0.0001 Acc: 1.0 Time: 48.41990780830383\nVal: Loss: 2.3243 Acc: 0.7266 lr: 0.000028\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nepochs = range(1, num_epoch+1)\n\nplt.title('Loss')\nplt.plot(epochs, hist['loss'],'ro', label='train')\nplt.plot(epochs, hist['val_loss'], 'r',label='val')\nplt.legend()\n\nplt.savefig('./mobv2_loss.png')\n\nplt.figure()\nplt.title('Acc')\nplt.plot(epochs, hist['acc'], 'bo', label='train')\nplt.plot(epochs, hist['val_acc', 'b', label='val'])\nplt.legend()\n\nplt.savefig('./mobv2_acc.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\nOptimizer: Adam\nSituation:\nMobileNetv2 doesn't learn anything until 30 epochs in the first try(batch_size=1024, T_max=num_epoch/2)\nBut, in the second try(batch_size=5096), model learns a bit. \nAgain, if batch_size=1024, the model doesn't learn anything.\n\nPossible Causes:\n\n1. T_max = num_epoch/2 causes this bad situation...? -> model stuck in the bad local optima.\n2. Something wrong with code...?\n3. lr=1e-3 is too big...?\n4. lr=1e-3 is too small...?\n5. Cosine Annealing LRScheduler should work with SGD as the original paper...? -> CosineAnnealingLR doesn't go well with Adam...?\n\nExperiments:\n\n1. T_max = num_epoch\n2. ResNet50 instead of MobileNetv2\n3. \n  * T_max = num_epoch/10 == 20\n  * **lr=1e-5** \n4. Init lr=1e-1\n\nResults:\n\n1. Nothing happens.(Still doesn't learn)\n2. ResNet50 Learns -> Code is fine...\n3. \n  * Nothing happens.\n  * **MobileNetv2 Learns!!**  \n    Epoch 93: Loss: 0.0002 Acc: 1.0 Time: 48.925941944122314  \n    Val: Loss: 2.3194 Acc: 0.7276 lr: 0.000028  \n4. Nothing happens"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}