{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x_1 * x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tokunagamasaomi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1,30,(10000, 2)).type(torch.FloatTensor)\n",
    "y = torch.stack([torch.tensor(x_1*x_2) for [x_1,x_2] in x])\n",
    "num_train = int(len(x)*2/3)\n",
    "train_x,train_y = x[:num_train,:],y[:num_train]\n",
    "test_x,test_y = x[num_train:,:],y[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16,8),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(2,1)\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8,1)\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 12044.9853\n",
      "Val: Loss: 1597.8493\n",
      "Epoch 2: Loss: 1087.3095\n",
      "Val: Loss: 581.5103\n",
      "Epoch 3: Loss: 578.2425\n",
      "Val: Loss: 430.6457\n",
      "Epoch 4: Loss: 450.3344\n",
      "Val: Loss: 331.8509\n",
      "Epoch 5: Loss: 328.8659\n",
      "Val: Loss: 239.6527\n",
      "Epoch 6: Loss: 214.0399\n",
      "Val: Loss: 160.6423\n",
      "Epoch 7: Loss: 146.6613\n",
      "Val: Loss: 101.0157\n",
      "Epoch 8: Loss: 123.0485\n",
      "Val: Loss: 90.6527\n",
      "Epoch 9: Loss: 111.6339\n",
      "Val: Loss: 80.3632\n",
      "Epoch 10: Loss: 103.7948\n",
      "Val: Loss: 76.6925\n",
      "Epoch 11: Loss: 98.1081\n",
      "Val: Loss: 71.7017\n",
      "Epoch 12: Loss: 81.6216\n",
      "Val: Loss: 55.1896\n",
      "Epoch 13: Loss: 67.8575\n",
      "Val: Loss: 36.7047\n",
      "Epoch 14: Loss: 61.1177\n",
      "Val: Loss: 33.0022\n",
      "Epoch 15: Loss: 56.6037\n",
      "Val: Loss: 29.2205\n",
      "Epoch 16: Loss: 53.9184\n",
      "Val: Loss: 28.1211\n",
      "Epoch 17: Loss: 51.6554\n",
      "Val: Loss: 26.1722\n",
      "Epoch 18: Loss: 50.4923\n",
      "Val: Loss: 24.8787\n",
      "Epoch 19: Loss: 49.1769\n",
      "Val: Loss: 24.9610\n",
      "Epoch 20: Loss: 47.8909\n",
      "Val: Loss: 25.4612\n",
      "Epoch 21: Loss: 47.0275\n",
      "Val: Loss: 23.8775\n",
      "Epoch 22: Loss: 45.7255\n",
      "Val: Loss: 25.4198\n",
      "Epoch 23: Loss: 44.3590\n",
      "Val: Loss: 27.4259\n",
      "Epoch 24: Loss: 43.3292\n",
      "Val: Loss: 26.8081\n",
      "Epoch 25: Loss: 41.9390\n",
      "Val: Loss: 26.0092\n",
      "Epoch 26: Loss: 41.1038\n",
      "Val: Loss: 26.3923\n",
      "Epoch 27: Loss: 40.2356\n",
      "Val: Loss: 24.2453\n",
      "Epoch 28: Loss: 39.3042\n",
      "Val: Loss: 23.8384\n",
      "Epoch 29: Loss: 38.3200\n",
      "Val: Loss: 25.0415\n",
      "Epoch 30: Loss: 37.5587\n",
      "Val: Loss: 24.6618\n",
      "Epoch 31: Loss: 36.9625\n",
      "Val: Loss: 25.0609\n",
      "Epoch 32: Loss: 36.0667\n",
      "Val: Loss: 22.8665\n",
      "Epoch 33: Loss: 35.0110\n",
      "Val: Loss: 21.3226\n",
      "Epoch 34: Loss: 34.1949\n",
      "Val: Loss: 20.2155\n",
      "Epoch 35: Loss: 33.9833\n",
      "Val: Loss: 19.1940\n",
      "Epoch 36: Loss: 33.5680\n",
      "Val: Loss: 19.9608\n",
      "Epoch 37: Loss: 33.3066\n",
      "Val: Loss: 19.1930\n",
      "Epoch 38: Loss: 32.8534\n",
      "Val: Loss: 18.9402\n",
      "Epoch 39: Loss: 31.7210\n",
      "Val: Loss: 15.4457\n",
      "Epoch 40: Loss: 29.0595\n",
      "Val: Loss: 19.4132\n",
      "Epoch 41: Loss: 28.2717\n",
      "Val: Loss: 19.7200\n",
      "Epoch 42: Loss: 27.7591\n",
      "Val: Loss: 15.5391\n",
      "Epoch 43: Loss: 27.8869\n",
      "Val: Loss: 10.7608\n",
      "Epoch 44: Loss: 27.0294\n",
      "Val: Loss: 10.3082\n",
      "Epoch 45: Loss: 26.5124\n",
      "Val: Loss: 11.1043\n",
      "Epoch 46: Loss: 25.1759\n",
      "Val: Loss: 10.2304\n",
      "Epoch 47: Loss: 25.2639\n",
      "Val: Loss: 10.9233\n",
      "Epoch 48: Loss: 23.7412\n",
      "Val: Loss: 8.4077\n",
      "Epoch 49: Loss: 22.6272\n",
      "Val: Loss: 8.0378\n",
      "Epoch 50: Loss: 23.1251\n",
      "Val: Loss: 10.1318\n",
      "Epoch 51: Loss: 22.6971\n",
      "Val: Loss: 7.2196\n",
      "Epoch 52: Loss: 21.9564\n",
      "Val: Loss: 11.0376\n",
      "Epoch 53: Loss: 22.1540\n",
      "Val: Loss: 15.4160\n",
      "Epoch 54: Loss: 22.1677\n",
      "Val: Loss: 22.0972\n",
      "Epoch 55: Loss: 22.2898\n",
      "Val: Loss: 12.4429\n",
      "Epoch 56: Loss: 22.5086\n",
      "Val: Loss: 14.4759\n",
      "Epoch 57: Loss: 21.9476\n",
      "Val: Loss: 13.0472\n",
      "Epoch 58: Loss: 21.8794\n",
      "Val: Loss: 12.1854\n",
      "Epoch 59: Loss: 21.9774\n",
      "Val: Loss: 15.7004\n",
      "Epoch 60: Loss: 21.7350\n",
      "Val: Loss: 15.6737\n",
      "Epoch 61: Loss: 20.8683\n",
      "Val: Loss: 13.2011\n",
      "Epoch 62: Loss: 20.4395\n",
      "Val: Loss: 13.9530\n",
      "Epoch 63: Loss: 20.7661\n",
      "Val: Loss: 12.9243\n",
      "Epoch 64: Loss: 20.6471\n",
      "Val: Loss: 10.8359\n",
      "Epoch 65: Loss: 20.8317\n",
      "Val: Loss: 12.6965\n",
      "Epoch 66: Loss: 19.1208\n",
      "Val: Loss: 13.2121\n",
      "Epoch 67: Loss: 19.2751\n",
      "Val: Loss: 14.7667\n",
      "Epoch 68: Loss: 19.6628\n",
      "Val: Loss: 15.3764\n",
      "Epoch 69: Loss: 19.5908\n",
      "Val: Loss: 16.5055\n",
      "Epoch 70: Loss: 19.0311\n",
      "Val: Loss: 14.6413\n",
      "Epoch 71: Loss: 20.9249\n",
      "Val: Loss: 9.8988\n",
      "Epoch 72: Loss: 18.7528\n",
      "Val: Loss: 12.5434\n",
      "Epoch 73: Loss: 19.3975\n",
      "Val: Loss: 11.6325\n",
      "Epoch 74: Loss: 20.8054\n",
      "Val: Loss: 8.3798\n",
      "Epoch 75: Loss: 20.7144\n",
      "Val: Loss: 8.5030\n",
      "Epoch 76: Loss: 19.3653\n",
      "Val: Loss: 10.6943\n",
      "Epoch 77: Loss: 19.6074\n",
      "Val: Loss: 8.8053\n",
      "Epoch 78: Loss: 18.1562\n",
      "Val: Loss: 17.2383\n",
      "Epoch 79: Loss: 24.0868\n",
      "Val: Loss: 32.4006\n",
      "Epoch 80: Loss: 20.2957\n",
      "Val: Loss: 19.4566\n",
      "Epoch 81: Loss: 19.7152\n",
      "Val: Loss: 19.6941\n",
      "Epoch 82: Loss: 19.2199\n",
      "Val: Loss: 17.8565\n",
      "Epoch 83: Loss: 18.2712\n",
      "Val: Loss: 16.3336\n",
      "Epoch 84: Loss: 18.5056\n",
      "Val: Loss: 11.6996\n",
      "Epoch 85: Loss: 18.3515\n",
      "Val: Loss: 14.3522\n",
      "Epoch 86: Loss: 18.4872\n",
      "Val: Loss: 16.2505\n",
      "Epoch 87: Loss: 17.2372\n",
      "Val: Loss: 18.1367\n",
      "Epoch 88: Loss: 16.9843\n",
      "Val: Loss: 18.9984\n",
      "Epoch 89: Loss: 16.7900\n",
      "Val: Loss: 19.2322\n",
      "Epoch 90: Loss: 15.9848\n",
      "Val: Loss: 19.5167\n",
      "Epoch 91: Loss: 15.6709\n",
      "Val: Loss: 17.6517\n",
      "Epoch 92: Loss: 15.5818\n",
      "Val: Loss: 17.9593\n",
      "Epoch 93: Loss: 15.5218\n",
      "Val: Loss: 17.9027\n",
      "Epoch 94: Loss: 15.7396\n",
      "Val: Loss: 17.3573\n",
      "Epoch 95: Loss: 15.6550\n",
      "Val: Loss: 16.4306\n",
      "Epoch 96: Loss: 15.6140\n",
      "Val: Loss: 16.5644\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-8646a09062c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import tqdm\n",
    "num_epochs = 10\n",
    "# c = 0.\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    l = 0.\n",
    "    for data,label in zip(train_x,train_y):\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        l += loss.item()\n",
    "        #     c += 1 if output == label else 0\n",
    "\n",
    "    print(\"Epoch {}: Loss: {:.4f}\".format(epoch,l/len(train_x)))\n",
    "    \n",
    "    l = 0.\n",
    "    for data, label in zip(test_x, test_y):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        l += loss.item()\n",
    "        \n",
    "    print(\"Val: Loss: {:.4f}\".format(l/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.5928], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor([80,20.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosLog(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosLog,self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        zeros = torch.zeros_like(x)\n",
    "        return torch.where(x>0,torch.log(x),zeros)\n",
    "    \n",
    "class Exp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Exp,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.exp(x)\n",
    "        \n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,16),\n",
    "    PosLog(),\n",
    "    torch.nn.Linear(16,8),\n",
    "    Exp(),\n",
    "    torch.nn.Linear(8,1)\n",
    ")\n",
    "\n",
    "lr = 3e-4\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 11634.6344\n",
      "set net!\n",
      "Val: Loss: 527.7053\n",
      "Epoch 2: Loss: 179.0129\n",
      "set net!\n",
      "Val: Loss: 55.3564\n",
      "Epoch 3: Loss: 63.8768\n",
      "set net!\n",
      "Val: Loss: 54.2901\n",
      "Epoch 4: Loss: 34.6135\n",
      "set net!\n",
      "Val: Loss: 18.7632\n",
      "Epoch 5: Loss: 32.0313\n",
      "Val: Loss: 36.6153\n",
      "Epoch 6: Loss: 27.7576\n",
      "set net!\n",
      "Val: Loss: 14.9711\n",
      "Epoch 7: Loss: 21.1694\n",
      "Val: Loss: 15.9908\n",
      "Epoch 8: Loss: 23.7410\n",
      "set net!\n",
      "Val: Loss: 9.1980\n",
      "Epoch 9: Loss: 20.0086\n",
      "Val: Loss: 19.8688\n",
      "Epoch 10: Loss: 12.4639\n",
      "set net!\n",
      "Val: Loss: 5.2778\n"
     ]
    }
   ],
   "source": [
    "# import tqdm\n",
    "num_epochs = 10\n",
    "l_min = float(\"inf\")\n",
    "# c = 0.\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    l = 0.\n",
    "    for data,label in zip(train_x,train_y):\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        l += loss.item()\n",
    "        #     c += 1 if output == label else 0\n",
    "\n",
    "    print(\"Epoch {}: Loss: {:.4f}\".format(epoch,l/len(train_x)))\n",
    "    \n",
    "    l = 0.\n",
    "    for data, label in zip(test_x, test_y):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        l += loss.item()\n",
    "        \n",
    "    if l < l_min:\n",
    "        print(\"set net!\")\n",
    "        l_min = l\n",
    "        min_net = copy.deepcopy(net)\n",
    "        \n",
    "    print(\"Val: Loss: {:.4f}\".format(l/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2482.7644], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_net(torch.tensor([80.,30.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x_1 + x_2 + x_1 * x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tokunagamasaomi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1,30,(10000, 2)).type(torch.FloatTensor)\n",
    "y = torch.stack([torch.tensor(x_1+x_2+x_1*x_2) for [x_1,x_2] in x])\n",
    "num_train = int(len(x)*2/3)\n",
    "train_x,train_y = x[:num_train,:],y[:num_train]\n",
    "test_x,test_y = x[num_train:,:],y[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 4798.1226\n",
      "Val: Loss: 1155.7575\n",
      "Epoch 2: Loss: 687.7025\n",
      "Val: Loss: 354.7247\n",
      "Epoch 3: Loss: 283.8792\n",
      "Val: Loss: 293.1387\n",
      "Epoch 4: Loss: 166.6305\n",
      "Val: Loss: 120.7449\n",
      "Epoch 5: Loss: 136.9444\n",
      "Val: Loss: 89.0698\n",
      "Epoch 6: Loss: 125.0764\n",
      "Val: Loss: 173.4915\n",
      "Epoch 7: Loss: 120.2337\n",
      "Val: Loss: 143.1009\n",
      "Epoch 8: Loss: 109.9115\n",
      "Val: Loss: 267.7850\n",
      "Epoch 9: Loss: 98.3847\n",
      "Val: Loss: 296.7559\n",
      "Epoch 10: Loss: 96.0563\n",
      "Val: Loss: 253.5187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16,8),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(2,1)\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8,1)\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=3e-3)\n",
    "\n",
    "# import tqdm\n",
    "num_epochs = 10\n",
    "# c = 0.\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    l = 0.\n",
    "    for data,label in zip(train_x,train_y):\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        l += loss.item()\n",
    "        #     c += 1 if output == label else 0\n",
    "\n",
    "    print(\"Epoch {}: Loss: {:.4f}\".format(epoch,l/len(train_x)))\n",
    "    \n",
    "    l = 0.\n",
    "    for data, label in zip(test_x, test_y):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        l += loss.item()\n",
    "        \n",
    "    print(\"Val: Loss: {:.4f}\".format(l/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 7972.5173\n",
      "set net!\n",
      "Val: Loss: 115.9319\n",
      "Epoch 2: Loss: 45.0892\n",
      "set net!\n",
      "Val: Loss: 25.1929\n",
      "Epoch 3: Loss: 19.9456\n",
      "set net!\n",
      "Val: Loss: 21.1416\n",
      "Epoch 4: Loss: 14.2337\n",
      "set net!\n",
      "Val: Loss: 14.9519\n",
      "Epoch 5: Loss: 11.8240\n",
      "set net!\n",
      "Val: Loss: 12.8000\n",
      "Epoch 6: Loss: 10.8230\n",
      "set net!\n",
      "Val: Loss: 11.5280\n",
      "Epoch 7: Loss: 9.8705\n",
      "set net!\n",
      "Val: Loss: 6.5083\n",
      "Epoch 8: Loss: 8.6705\n",
      "set net!\n",
      "Val: Loss: 4.9333\n",
      "Epoch 9: Loss: 8.3882\n",
      "set net!\n",
      "Val: Loss: 4.1149\n",
      "Epoch 10: Loss: 7.3028\n",
      "set net!\n",
      "Val: Loss: 3.0981\n"
     ]
    }
   ],
   "source": [
    "class PosLog(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosLog,self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        zeros = torch.zeros_like(x)\n",
    "        return torch.where(x>0,torch.log(x),zeros)\n",
    "    \n",
    "class Exp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Exp,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.exp(x)\n",
    "        \n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,16),\n",
    "    PosLog(),\n",
    "    torch.nn.Linear(16,8),\n",
    "    Exp(),\n",
    "    torch.nn.Linear(8,1)\n",
    ")\n",
    "\n",
    "lr = 3e-4\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "\n",
    "# import tqdm\n",
    "num_epochs = 10\n",
    "l_min = float(\"inf\")\n",
    "# c = 0.\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    l = 0.\n",
    "    for data,label in zip(train_x,train_y):\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        l += loss.item()\n",
    "        #     c += 1 if output == label else 0\n",
    "\n",
    "    print(\"Epoch {}: Loss: {:.4f}\".format(epoch,l/len(train_x)))\n",
    "    \n",
    "    l = 0.\n",
    "    for data, label in zip(test_x, test_y):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        l += loss.item()\n",
    "        \n",
    "    if l < l_min:\n",
    "        print(\"set net!\")\n",
    "        l_min = l\n",
    "        min_net = copy.deepcopy(net)\n",
    "        \n",
    "    print(\"Val: Loss: {:.4f}\".format(l/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([149.9834], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_net(torch.tensor([50.,2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tokunagamasaomi/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# x = torch.randint(0,1,(10000, 10)).type(torch.FloatTensor)\n",
    "x = torch.FloatTensor(10000, 10).uniform_(0,1)\n",
    "y = torch.stack([torch.tensor((1/(1+x_1**2 + x_2**2 + x_3**2)) + torch.sqrt(torch.exp(x_4+x_5)) + torch.abs(x_6 + x_7) + x_8*x_9*x_10 ) for [x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10] in x])\n",
    "num_train = int(len(x)*2/3)\n",
    "train_x,train_y = x[:num_train,:],y[:num_train]\n",
    "test_x,test_y = x[num_train:,:],y[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.0369\n",
      "Val: Loss: 0.0864\n",
      "Epoch 2: Loss: 0.0097\n",
      "Val: Loss: 0.0121\n",
      "Epoch 3: Loss: 0.0061\n",
      "Val: Loss: 0.0069\n",
      "Epoch 4: Loss: 0.0044\n",
      "Val: Loss: 0.0016\n",
      "Epoch 5: Loss: 0.0037\n",
      "Val: Loss: 0.0010\n",
      "Epoch 6: Loss: 0.0033\n",
      "Val: Loss: 0.0011\n",
      "Epoch 7: Loss: 0.0029\n",
      "Val: Loss: 0.0007\n",
      "Epoch 8: Loss: 0.0028\n",
      "Val: Loss: 0.0012\n",
      "Epoch 9: Loss: 0.0023\n",
      "Val: Loss: 0.0006\n",
      "Epoch 10: Loss: 0.0021\n",
      "Val: Loss: 0.0006\n",
      "Epoch 11: Loss: 0.0019\n",
      "Val: Loss: 0.0008\n",
      "Epoch 12: Loss: 0.0018\n",
      "Val: Loss: 0.0007\n",
      "Epoch 13: Loss: 0.0017\n",
      "Val: Loss: 0.0008\n",
      "Epoch 14: Loss: 0.0015\n",
      "Val: Loss: 0.0010\n",
      "Epoch 15: Loss: 0.0015\n",
      "Val: Loss: 0.0006\n",
      "Epoch 16: Loss: 0.0014\n",
      "Val: Loss: 0.0011\n",
      "Epoch 17: Loss: 0.0014\n",
      "Val: Loss: 0.0010\n",
      "Epoch 18: Loss: 0.0014\n",
      "Val: Loss: 0.0006\n",
      "Epoch 19: Loss: 0.0013\n",
      "Val: Loss: 0.0007\n",
      "Epoch 20: Loss: 0.0012\n",
      "Val: Loss: 0.0006\n",
      "Epoch 21: Loss: 0.0012\n",
      "Val: Loss: 0.0006\n",
      "Epoch 22: Loss: 0.0012\n",
      "Val: Loss: 0.0007\n",
      "Epoch 23: Loss: 0.0011\n",
      "Val: Loss: 0.0006\n",
      "Epoch 24: Loss: 0.0011\n",
      "Val: Loss: 0.0006\n",
      "Epoch 25: Loss: 0.0011\n",
      "Val: Loss: 0.0006\n",
      "Epoch 26: Loss: 0.0009\n",
      "Val: Loss: 0.0005\n",
      "Epoch 27: Loss: 0.0010\n",
      "Val: Loss: 0.0008\n",
      "Epoch 28: Loss: 0.0009\n",
      "Val: Loss: 0.0010\n",
      "Epoch 29: Loss: 0.0009\n",
      "Val: Loss: 0.0004\n",
      "Epoch 30: Loss: 0.0009\n",
      "Val: Loss: 0.0004\n",
      "Epoch 31: Loss: 0.0009\n",
      "Val: Loss: 0.0005\n",
      "Epoch 32: Loss: 0.0008\n",
      "Val: Loss: 0.0005\n",
      "Epoch 33: Loss: 0.0009\n",
      "Val: Loss: 0.0006\n",
      "Epoch 34: Loss: 0.0008\n",
      "Val: Loss: 0.0005\n",
      "Epoch 35: Loss: 0.0008\n",
      "Val: Loss: 0.0004\n",
      "Epoch 36: Loss: 0.0008\n",
      "Val: Loss: 0.0004\n",
      "Epoch 37: Loss: 0.0008\n",
      "Val: Loss: 0.0006\n",
      "Epoch 38: Loss: 0.0008\n",
      "Val: Loss: 0.0005\n",
      "Epoch 39: Loss: 0.0008\n",
      "Val: Loss: 0.0006\n",
      "Epoch 40: Loss: 0.0008\n",
      "Val: Loss: 0.0006\n",
      "Epoch 41: Loss: 0.0008\n",
      "Val: Loss: 0.0004\n",
      "Epoch 42: Loss: 0.0008\n",
      "Val: Loss: 0.0007\n",
      "Epoch 43: Loss: 0.0008\n",
      "Val: Loss: 0.0007\n",
      "Epoch 44: Loss: 0.0007\n",
      "Val: Loss: 0.0006\n",
      "Epoch 45: Loss: 0.0008\n",
      "Val: Loss: 0.0007\n",
      "Epoch 46: Loss: 0.0007\n",
      "Val: Loss: 0.0008\n",
      "Epoch 47: Loss: 0.0007\n",
      "Val: Loss: 0.0006\n",
      "Epoch 48: Loss: 0.0007\n",
      "Val: Loss: 0.0006\n",
      "Epoch 49: Loss: 0.0007\n",
      "Val: Loss: 0.0006\n",
      "Epoch 50: Loss: 0.0007\n",
      "Val: Loss: 0.0012\n",
      "Epoch 51: Loss: 0.0007\n",
      "Val: Loss: 0.0007\n",
      "Epoch 52: Loss: 0.0007\n",
      "Val: Loss: 0.0005\n",
      "Epoch 53: Loss: 0.0007\n",
      "Val: Loss: 0.0006\n",
      "Epoch 54: Loss: 0.0008\n",
      "Val: Loss: 0.0007\n",
      "Epoch 55: Loss: 0.0006\n",
      "Val: Loss: 0.0009\n",
      "Epoch 56: Loss: 0.0007\n",
      "Val: Loss: 0.0006\n",
      "Epoch 57: Loss: 0.0007\n",
      "Val: Loss: 0.0003\n",
      "Epoch 58: Loss: 0.0007\n",
      "Val: Loss: 0.0005\n",
      "Epoch 59: Loss: 0.0006\n",
      "Val: Loss: 0.0003\n",
      "Epoch 60: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 61: Loss: 0.0006\n",
      "Val: Loss: 0.0003\n",
      "Epoch 62: Loss: 0.0007\n",
      "Val: Loss: 0.0012\n",
      "Epoch 63: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 64: Loss: 0.0007\n",
      "Val: Loss: 0.0004\n",
      "Epoch 65: Loss: 0.0007\n",
      "Val: Loss: 0.0008\n",
      "Epoch 66: Loss: 0.0006\n",
      "Val: Loss: 0.0023\n",
      "Epoch 67: Loss: 0.0006\n",
      "Val: Loss: 0.0012\n",
      "Epoch 68: Loss: 0.0006\n",
      "Val: Loss: 0.0018\n",
      "Epoch 69: Loss: 0.0006\n",
      "Val: Loss: 0.0011\n",
      "Epoch 70: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 71: Loss: 0.0006\n",
      "Val: Loss: 0.0015\n",
      "Epoch 72: Loss: 0.0006\n",
      "Val: Loss: 0.0004\n",
      "Epoch 73: Loss: 0.0006\n",
      "Val: Loss: 0.0004\n",
      "Epoch 74: Loss: 0.0006\n",
      "Val: Loss: 0.0008\n",
      "Epoch 75: Loss: 0.0007\n",
      "Val: Loss: 0.0012\n",
      "Epoch 76: Loss: 0.0006\n",
      "Val: Loss: 0.0015\n",
      "Epoch 77: Loss: 0.0006\n",
      "Val: Loss: 0.0004\n",
      "Epoch 78: Loss: 0.0006\n",
      "Val: Loss: 0.0005\n",
      "Epoch 79: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 80: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 81: Loss: 0.0006\n",
      "Val: Loss: 0.0016\n",
      "Epoch 82: Loss: 0.0007\n",
      "Val: Loss: 0.0008\n",
      "Epoch 83: Loss: 0.0006\n",
      "Val: Loss: 0.0011\n",
      "Epoch 84: Loss: 0.0007\n",
      "Val: Loss: 0.0011\n",
      "Epoch 85: Loss: 0.0006\n",
      "Val: Loss: 0.0018\n",
      "Epoch 86: Loss: 0.0007\n",
      "Val: Loss: 0.0019\n",
      "Epoch 87: Loss: 0.0007\n",
      "Val: Loss: 0.0011\n",
      "Epoch 88: Loss: 0.0006\n",
      "Val: Loss: 0.0008\n",
      "Epoch 89: Loss: 0.0006\n",
      "Val: Loss: 0.0009\n",
      "Epoch 90: Loss: 0.0006\n",
      "Val: Loss: 0.0009\n",
      "Epoch 91: Loss: 0.0007\n",
      "Val: Loss: 0.0019\n",
      "Epoch 92: Loss: 0.0007\n",
      "Val: Loss: 0.0014\n",
      "Epoch 93: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 94: Loss: 0.0006\n",
      "Val: Loss: 0.0006\n",
      "Epoch 95: Loss: 0.0006\n",
      "Val: Loss: 0.0019\n",
      "Epoch 96: Loss: 0.0006\n",
      "Val: Loss: 0.0016\n",
      "Epoch 97: Loss: 0.0006\n",
      "Val: Loss: 0.0015\n",
      "Epoch 98: Loss: 0.0006\n",
      "Val: Loss: 0.0012\n",
      "Epoch 99: Loss: 0.0006\n",
      "Val: Loss: 0.0011\n",
      "Epoch 100: Loss: 0.0006\n",
      "Val: Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10,256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256,128),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(2,1)\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,1)\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=3e-3)\n",
    "\n",
    "# import tqdm\n",
    "num_epochs = 100\n",
    "# c = 0.\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    l = 0.\n",
    "    for data,label in zip(train_x,train_y):\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        l += loss.item()\n",
    "        #     c += 1 if output == label else 0\n",
    "\n",
    "    print(\"Epoch {}: Loss: {:.4f}\".format(epoch,l/len(train_x)))\n",
    "    \n",
    "    l = 0.\n",
    "    for data, label in zip(test_x, test_y):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        l += loss.item()\n",
    "        \n",
    "    print(\"Val: Loss: {:.4f}\".format(l/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.1085\n",
      "set net!\n",
      "Val: Loss: 0.0238\n",
      "Epoch 2: Loss: 0.0401\n",
      "set net!\n",
      "Val: Loss: 0.0211\n",
      "Epoch 3: Loss: 0.0244\n",
      "Val: Loss: 0.0663\n",
      "Epoch 4: Loss: 0.0164\n",
      "set net!\n",
      "Val: Loss: 0.0088\n",
      "Epoch 5: Loss: 0.0111\n",
      "set net!\n",
      "Val: Loss: 0.0060\n",
      "Epoch 6: Loss: 0.0073\n",
      "set net!\n",
      "Val: Loss: 0.0030\n",
      "Epoch 7: Loss: 0.0051\n",
      "Val: Loss: 0.0031\n",
      "Epoch 8: Loss: 0.0040\n",
      "set net!\n",
      "Val: Loss: 0.0026\n",
      "Epoch 9: Loss: 0.0038\n",
      "Val: Loss: 0.0098\n",
      "Epoch 10: Loss: 0.0042\n",
      "Val: Loss: 0.0256\n",
      "Epoch 11: Loss: 0.0034\n",
      "Val: Loss: 0.0152\n",
      "Epoch 12: Loss: 0.0034\n",
      "Val: Loss: 0.0038\n",
      "Epoch 13: Loss: 0.0031\n",
      "Val: Loss: 0.0051\n",
      "Epoch 14: Loss: 0.0028\n",
      "Val: Loss: 0.0096\n",
      "Epoch 15: Loss: 0.0026\n",
      "set net!\n",
      "Val: Loss: 0.0021\n",
      "Epoch 16: Loss: 0.0027\n",
      "Val: Loss: 0.0035\n",
      "Epoch 17: Loss: 0.0023\n",
      "Val: Loss: 0.0065\n",
      "Epoch 18: Loss: 0.0024\n",
      "Val: Loss: 0.0068\n",
      "Epoch 19: Loss: 0.0023\n",
      "Val: Loss: 0.0039\n",
      "Epoch 20: Loss: 0.0019\n",
      "Val: Loss: 0.0030\n",
      "Epoch 21: Loss: 0.0018\n",
      "Val: Loss: 0.0034\n",
      "Epoch 22: Loss: 0.0017\n",
      "set net!\n",
      "Val: Loss: 0.0012\n",
      "Epoch 23: Loss: 0.0019\n",
      "Val: Loss: 0.0065\n",
      "Epoch 24: Loss: 0.0019\n",
      "Val: Loss: 0.0041\n",
      "Epoch 25: Loss: 0.0015\n",
      "Val: Loss: 0.0070\n",
      "Epoch 26: Loss: 0.0017\n",
      "Val: Loss: 0.0024\n",
      "Epoch 27: Loss: 0.0016\n",
      "Val: Loss: 0.0021\n",
      "Epoch 28: Loss: 0.0448\n",
      "Val: Loss: 0.0015\n",
      "Epoch 29: Loss: 0.0015\n",
      "Val: Loss: 0.0023\n",
      "Epoch 30: Loss: 0.0015\n",
      "Val: Loss: 0.0030\n",
      "Epoch 31: Loss: 0.0013\n",
      "set net!\n",
      "Val: Loss: 0.0010\n",
      "Epoch 32: Loss: 0.0014\n",
      "Val: Loss: 0.0076\n",
      "Epoch 33: Loss: 0.0013\n",
      "Val: Loss: 0.0011\n",
      "Epoch 34: Loss: 0.0012\n",
      "set net!\n",
      "Val: Loss: 0.0007\n",
      "Epoch 35: Loss: 0.0011\n",
      "Val: Loss: 0.0027\n",
      "Epoch 36: Loss: 0.0015\n",
      "Val: Loss: 0.0019\n",
      "Epoch 37: Loss: 0.0012\n",
      "Val: Loss: 0.0011\n",
      "Epoch 38: Loss: 0.0011\n",
      "Val: Loss: 0.0022\n",
      "Epoch 39: Loss: 0.0012\n",
      "Val: Loss: 0.0012\n",
      "Epoch 40: Loss: 0.0011\n",
      "Val: Loss: 0.0009\n",
      "Epoch 41: Loss: 0.0012\n",
      "Val: Loss: 0.0041\n",
      "Epoch 42: Loss: 0.0016\n",
      "Val: Loss: 0.0039\n",
      "Epoch 43: Loss: 0.0011\n",
      "Val: Loss: 0.0013\n",
      "Epoch 44: Loss: 0.0013\n",
      "Val: Loss: 0.0014\n",
      "Epoch 45: Loss: 0.0010\n",
      "set net!\n",
      "Val: Loss: 0.0006\n",
      "Epoch 46: Loss: 0.0009\n",
      "Val: Loss: 0.0019\n",
      "Epoch 47: Loss: 0.0010\n",
      "Val: Loss: 0.0009\n",
      "Epoch 48: Loss: 0.0009\n",
      "Val: Loss: 0.0008\n",
      "Epoch 49: Loss: 0.0009\n",
      "Val: Loss: 0.0011\n",
      "Epoch 50: Loss: 0.0009\n",
      "Val: Loss: 0.0030\n",
      "Epoch 51: Loss: 0.0011\n",
      "Val: Loss: 0.0016\n",
      "Epoch 52: Loss: 0.0009\n",
      "Val: Loss: 0.0031\n",
      "Epoch 53: Loss: 0.0008\n",
      "Val: Loss: 0.0009\n",
      "Epoch 54: Loss: 0.0008\n",
      "Val: Loss: 0.0010\n",
      "Epoch 55: Loss: 0.0007\n",
      "Val: Loss: 0.0008\n",
      "Epoch 56: Loss: 0.0008\n",
      "Val: Loss: 0.0010\n",
      "Epoch 57: Loss: 0.0008\n",
      "Val: Loss: 0.0007\n",
      "Epoch 58: Loss: 0.0008\n",
      "Val: Loss: 0.0008\n",
      "Epoch 59: Loss: 0.0008\n",
      "Val: Loss: 0.0011\n",
      "Epoch 60: Loss: 0.0011\n",
      "Val: Loss: 0.0008\n",
      "Epoch 61: Loss: 0.0007\n",
      "set net!\n",
      "Val: Loss: 0.0005\n",
      "Epoch 62: Loss: 0.0023\n",
      "Val: Loss: 0.0009\n",
      "Epoch 63: Loss: 0.0007\n",
      "Val: Loss: 0.0008\n",
      "Epoch 64: Loss: 0.0007\n",
      "Val: Loss: 0.0011\n",
      "Epoch 65: Loss: 0.0008\n",
      "Val: Loss: 0.0017\n",
      "Epoch 66: Loss: 0.0007\n",
      "set net!\n",
      "Val: Loss: 0.0005\n",
      "Epoch 67: Loss: 0.0007\n",
      "set net!\n",
      "Val: Loss: 0.0003\n",
      "Epoch 68: Loss: 0.0007\n",
      "Val: Loss: 0.0018\n",
      "Epoch 69: Loss: 0.0007\n",
      "Val: Loss: 0.0012\n",
      "Epoch 70: Loss: 0.0007\n",
      "Val: Loss: 0.0010\n",
      "Epoch 71: Loss: 0.0007\n",
      "Val: Loss: 0.0004\n",
      "Epoch 72: Loss: 0.0023\n",
      "Val: Loss: 0.0011\n",
      "Epoch 73: Loss: 0.0006\n",
      "Val: Loss: 0.0009\n",
      "Epoch 74: Loss: 0.0009\n",
      "Val: Loss: 0.0009\n",
      "Epoch 75: Loss: 0.0006\n",
      "Val: Loss: 0.0004\n",
      "Epoch 76: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 77: Loss: 0.0006\n",
      "Val: Loss: 0.0009\n",
      "Epoch 78: Loss: 0.0007\n",
      "Val: Loss: 0.0005\n",
      "Epoch 79: Loss: 0.0008\n",
      "Val: Loss: 0.0007\n",
      "Epoch 80: Loss: 0.0006\n",
      "set net!\n",
      "Val: Loss: 0.0003\n",
      "Epoch 81: Loss: 0.0006\n",
      "Val: Loss: 0.0005\n",
      "Epoch 82: Loss: 0.0006\n",
      "Val: Loss: 0.0007\n",
      "Epoch 83: Loss: 0.0006\n",
      "Val: Loss: 0.0008\n",
      "Epoch 84: Loss: 0.0006\n",
      "Val: Loss: 0.0004\n",
      "Epoch 85: Loss: 0.0005\n",
      "Val: Loss: 0.0004\n",
      "Epoch 86: Loss: 0.0005\n",
      "Val: Loss: 0.0008\n",
      "Epoch 87: Loss: 0.0007\n",
      "Val: Loss: 0.0010\n",
      "Epoch 88: Loss: 0.0006\n",
      "Val: Loss: 0.0009\n",
      "Epoch 89: Loss: 0.0005\n",
      "Val: Loss: 0.0011\n",
      "Epoch 90: Loss: 0.0006\n",
      "Val: Loss: 0.0021\n",
      "Epoch 91: Loss: 0.0005\n",
      "Val: Loss: 0.0013\n",
      "Epoch 92: Loss: 0.0005\n",
      "Val: Loss: 0.0008\n",
      "Epoch 93: Loss: 0.0006\n",
      "Val: Loss: 0.0006\n",
      "Epoch 94: Loss: 0.0005\n",
      "Val: Loss: 0.0005\n",
      "Epoch 95: Loss: 0.0005\n",
      "Val: Loss: 0.0015\n",
      "Epoch 96: Loss: 0.0005\n",
      "Val: Loss: 0.0015\n",
      "Epoch 97: Loss: 0.0005\n",
      "Val: Loss: 0.0005\n",
      "Epoch 98: Loss: 0.0005\n",
      "Val: Loss: 0.0007\n",
      "Epoch 99: Loss: 0.0019\n",
      "set net!\n",
      "Val: Loss: 0.0003\n",
      "Epoch 100: Loss: 0.0005\n",
      "Val: Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "class PosLog(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosLog,self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        zeros = torch.zeros_like(x)\n",
    "        return torch.where(x>0,torch.log(x),zeros)\n",
    "    \n",
    "class Exp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Exp,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.exp(x)\n",
    "        \n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10,256),\n",
    "    PosLog(),\n",
    "    torch.nn.Linear(256,128),\n",
    "    Exp(),\n",
    "    torch.nn.Linear(128,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,1)\n",
    ")\n",
    "\n",
    "lr = 3e-4\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "\n",
    "# import tqdm\n",
    "num_epochs = 100\n",
    "l_min = float(\"inf\")\n",
    "# c = 0.\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    l = 0.\n",
    "    for data,label in zip(train_x,train_y):\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        l += loss.item()\n",
    "        #     c += 1 if output == label else 0\n",
    "\n",
    "    print(\"Epoch {}: Loss: {:.4f}\".format(epoch,l/len(train_x)))\n",
    "    \n",
    "    l = 0.\n",
    "    for data, label in zip(test_x, test_y):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        l += loss.item()\n",
    "        \n",
    "    if l < l_min:\n",
    "        print(\"set net!\")\n",
    "        l_min = l\n",
    "        min_net = copy.deepcopy(net)\n",
    "        \n",
    "    print(\"Val: Loss: {:.4f}\".format(l/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
